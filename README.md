# instagram-highload
ДЗ-1 по курсу Highload. Проектирование веб-сервиса Instagram

### 1. Тема
Instagram - приложение для обмена фотографиями с элементами социальной сети

* функционал по публикации фото, их комментированию, просмотру ленты
* 70% аудитории моложе 30 лет
* наиболее популярен в США, России и Бразилии

### 2. Планируемая нагрузка

* аудитория - 1 млрд. человек ([источник](https://tass.ru/ekonomika/5309458))
* около 500 млн. пользователей ежедневно ([источник](https://lpgenerator.ru/blog/2019/02/28/instagram-v-cifrah-statistika-na-2019-god/))
* среднее время, проведенное в Instagram - 28 мин./день ([источник](https://www.likeni.ru/analytics/statistika-po-instagram-kotoruyu-nuzhno-znat-k-2020-godu/))

Обычно, пользователь заходит на сайт по несколько раз в день.
Зная, что среднее время, проведенное в Instagram - 28 мин./день, предположим,
что пользователь заходит 7 раз в день, проводя на сайте по 4 минуты.

Теперь оценим трафик среднестатистического пользователя за одну сессию.

За 4 минуты пользования сайтом(просмотр ленты), я сделал 315 запросов и загрузил 33.8 мб данных.
Их них:
* на динамику - 38 запросов и 198 кб
  * на формирование ленты - 26 запросов и 126 кб
  * на комментарии - 12 запросов и 72 кб
* на картинки - 250 запросов и 31.7 мб
* на остальную статику - 29 запросов и 4.3 мб

Тогда за сутки среднестатистический пользователь:
* для динамики <br/>
  * для формирования ленты:
    * загрузит ```7 * 474 = 3129 кб``` данных <br/>
    * сделает ```7 * 31 = 217``` запросов
  * для просмотра комментариев: 
    * загрузит ```7 * 144 = 1008 кб``` данных <br/>
    * сделает ```7 * 24 = 168``` запроса

* для картинок <br/>
  * загрузит ```7 * 31.7.4 = 222 мб``` данных <br/>
  * сделает ```7 * 250 = 1750``` запросов

* для остальной статики <br/>
  * загрузит ```7 * 4.3 = 30.1 мб``` данных <br/>
  * сделает ```7 * 29 = 203``` запроса

Примем, что в день публикуется 100 млн. постов ([источник](https://lpgenerator.ru/blog/2019/02/28/instagram-v-cifrah-statistika-na-2019-god/)),
т.е. ~1158 постов в секунду.
Согласно источникам([1](https://www.likeni.ru/analytics/25-faktov-ob-instagram-kotorye-dolzhen-znat-kazhdyy/), [2](https://3dnews.ru/950769)),
при аудитории в 200 млн. пользователей, ежесекундно публиковалось 8500 лайков и 1000
комментариев. Тогда аудитория в 1 млрд. человек будет публиковать в 5 раз больше комментариев и лайков, т.е.
42 500 лайков и 5000 комментариев в секунду.
Примем, что в каждом посте публикуется по 1 фото, тогда объем переданных данных равен ~130 кб(учитывая средний размер
фото в 125 кб и размер записи в таблице постов в 332 байт).
При лайке поста передается 75 байт, а при публикации комментария ~ 500 байт. Получается:
* на публикацию постов ```(130 кб * 1158) * (8/1024/1024) = 1.2 Гбит/с```
* на публикацию комментариев ```(500 байт * 5000) * (8/1024/1024) = 19.1 Мбит/с```
* на публикацию лайков ```(75 байт * 42 500) * (8/1024/1024) = 24.4 Мбит/с```

Примем, что единовременная нагрузка на сервис равняется 80% ежедневной аудитории. Тогда, можем рассчитать среднюю 
нагрузку с учетом всех видов активностей пользователя:

Расcчитаем средний трафик:
* динамика:
  * формирование ленты и просмотр постов:```(3129 кб * 400 000 000) * (8/1024/1024) / (24 * 60 * 60) = 111 Гбит/с```
  * публикация постов: ```1.2 Гбит/с```
  * просмотр и публикация комментариев: ```(1008 кб * 400 000 000) * (8/1024/1024) / (24 * 60 * 60) + 19.1 Мбит/с = 36 Гбит/с```
  * публикация лайков: ```24.4 Мбит/с```
* картинки: ```(222 мб * 400 000 000) * (8/1024) / (24 * 60 * 60) = 8030 Гбит/c```
* остальная статика:```(30.1 мб * 400 000 000) * (8/1024) / (24 * 60 * 60) = 1089 Гбит/c```

Расcчитаем средний RPS:
* динамика: 
  * формирование ленты и просмотр постов:```(217 * 400 000 000) / (24 * 60 * 60) = 1 004 630 RPS```
  * публикация постов: ```1158 RPS```
  * просмотр и публикация комментариев: ```(168 * 400 000 000) / (24 * 60 * 60) + 5000 RPS = 782 778 RPS```
  * публикация лайков: ```42 500 RPS```
* картинки: ```(1750 * 400 000 000) / (24 * 60 * 60) = 8 101 802 RPS```
* остальная статика: ```(203 * 400 000 000) / (24 * 60 * 60) = 939 815 RPS```

Отдельно стоит рассмотреть сервис авторизации/регистрации. Согласно [источнику](https://www.likeni.ru/analytics/statistika-po-instagram-kotoruyu-nuzhno-znat-k-2020-godu/), 
прирост аудитории Instagram на 2019 год составил 7,3%. Это значит, что за год произойдет 73 000 000
запросов на регистрацию, т.е.<br>
```73 000 000 / (365 * 24 * 60 * 60) = 2.4 RPS```. <br>
Предположив, что один такой запрос передает ~400 байт информации, получим<br>
```(400 байт * 73 000 000) * (8/1024) / (365 * 24 * 60 * 60) = 7.2 Кбит/с```<br>
Так как аудитория проекта составляет 500 млн. человек и мы приняли, что в среднем каждый их них заходит на сайт по 4 
раза в день, получаем 2 000 000 000 запросов на авторизацию в день, т.е. примерно
```2 000 000 000 / (24 * 60 * 60) = 23 148 RPS```<br>
Однако, стоит учесть, что для каждого взаимодействия пользователя с бэкендами, необходимо подтверждение авторизации. 
Тогда, нагрузка на сервис авторизации/регистрации составит: 
```2.4 + 23 148 + 1 004 630 + 1158 + 782 778 + 2500 = 1 814 217 RPS```
Запрос на авторизацию в Instagram передает 154 байта по сети, тогда нагрузка составит около<br>
```(154 байт * 1 814 217 RPS) * (8/1024/1024/1024) = 2.1 Гбит/с```<br>
Нагрузками на регистрацию в данном случае пренебрежем, т.к. они малы в сравнении с авторизацией.

### 3. Логическая схема БД

![Схема бд](https://github.com/Alkirys/instagram-highload/blob/main/images/BD_logic.png "Схема бд")

### 4. Физические системы хранения

Для хранения данных о пользователях, постах, комментариях и лайках, я выбрал
СУБД PostgreSQL, как наиболее функциональную и надежную базу данных, имеющую хорошую поддержку
и инструменты шардинга.

Для ускорения работы БД будут созданы следующие индексы: для таблицы постов - составной индекс по полям user_id и time 
и индекс по полю user_id. Для таблицы комментариев - составной индекс по полям post_id и time и индексы по полям user_id и post_id.
Для таблицы пользователей - составной индекс по полям user_id и subscribres.

Для снижения нагрузки на базы PostgreSql, а также уменьшения времени ответа на запросы актуальных данных, данные 
таблиц постов за последние 30 дней и комментариев за последние 15 дней будут дополнительно храниться в Tarantool.

Для ускорения работы БД будут созданы следующие индексы: для спейса постов - первичный индекс по полю post_id, 
вторичный индекс по полю post_id, составной индекс по полям user_id и time. 
Для спейса комментариев - первичный индекс по полю comment_id, вторичные индексы по полям user_id и post_id и 
составной индекс по полям post_id и time.

Шардинг таблиц пользователей и постов может производиться по полю id. Для таблицы комментариев, шардинг по id не 
подойдет, так как комментарий привязан к соответствующему посту, поэтому таблицу комментариев имеет смысл шардить по 
полю post_id.

Активные сессии пользователей для обеспечения скорейшего доступа будут храниться в Redis.
Также, Redis имеет возможность master-slave репликацию и встроенное API для работы с
Memcached.

####Алгоритм выбора шарда для записи поста и последующего формирования ленты. 

Будем располагать реплики серверов с данными 
так, чтобы в каждом датацентре была хотя бы одна slave-реплика. Тогда, согласно п.7, учитывая, что планируется создание 
2 датацентров, получается, что в одном датацентре будет стоять по одной master и slave-реплике, а в другом - 1 
slave-реплика. 
Примем, что в каждом датацентре будут находиться сервера с номером, кратным определенному числу в интервале от 0 до N-1,
где N - число датацентров(для 2 датацентров получим, что в 1 будут сервера с нечетными номерами, а во 2 - с четными).
При создании поста, для выбора шарда, будем вычислять хеш-функцию от его id и брать остаток от деления
на общее количество серверов. Если полученное значение соответствует серверу, находящемуся в другом датацентре, будем 
добавлять 1 к числу. Полученное значение будет номером физического сервера, на котором будут лежать данные.
При формировании ленты для пользователя, сначала будет сделан запрос в таблицу пользователя за списком людей, на которых
подписан пользователь, а затем, на основе полученного списка и текущего времени - сделан запрос за списком постов
(сначала в Tarantool, т.к. там лежат посты за последние 30 дней, а затем, если необходимы посты за более ранний период -
в Postgres). В дальнейшем, шард, на котором лежит отдельный пост можно будет вычислить по расписанному выше алгоритму.

Примерный расчет объема фото:

Согласно источникам([1](https://tjournal.ru/flood/48865-instagram-thanksgiving), [2](https://ru.epicstars.com/full_history_of_instagam/)) 
при аудитории в 100 млн. пользователей, в Instagram было опубликовано 16 млрд. фото.
Рассчитаем примерное количество фото для аудитории в 1 млрд. человек<br>
```(1 млрд. * 16 млрд.) / 100 млн. = 160 млрд.```
<br>
Тогда, приняв средний размер фото равный 130 кб, для их хранения понадобится<br>
```130 кб * 160000000000 / 1024 / 1024 = 19 836 426 Гб```

Примерный расчет объема таблиц:

* Пользователи:<br>
Аудитория сервиса - 1 млрд. человек. Тогда одна запись в таблице займет:<br>
  ```4(user_id) + 128(username) + 128(password) + 128(email) + 128(avatar) + 4 * 150(subscribe) = 1116 байт```
  <br><br>
  А всего необходимо<br>
  ```1116 * 1000000000 / 1024^3  = 1040 Гб```<br><br>
  
* Посты:<br>
  Одна запись в таблице займет:<br>
  ```4(post_id) + 4(user_id) + 128(description) + 128(images_url) + 64(location) + 4(likes) + 8(time) = 340 байта```
  <br><br>
  Предположим, что каждый пост содержит одно фото, тогда всего таблица займет <br>
  ```340 * 160000000000 / 1024^3 = 50 664 Гб```<br><br>

* Комментарии:<br>
  Одна запись в таблице займет:<br>
  ```4(comment_id) + 4(user_id) + 4(post_id) + 128(message) + 8(time) = 148 байт```
  <br><br>
  Согласно [источнику](https://blog.cybermarketing.ru/vovlechennost-instagram/), медиана числа комментариев к посту равна 4, тогда таблица займет:<br>
  ```148 * 160000000000 * 4 / 1024^3  = 88 215 Гб```
  <br><br>

* Лайки:<br>
  Одна запись в таблице займет:<br>
  ```4(user_id) + 4(post_id) = 8 байт```
  <br><br>
  Согласно [источнику](https://blog.cybermarketing.ru/vovlechennost-instagram/), медиана количества лайков к посту равна 100, тогда таблица займет:<br>
  ```8 * 160000000000 * 100 / 1024^3  = 119 210 Гб```
  <br><br>
  
Таким образом, для хранения таблиц потребуется<br>
```1040 + 50 664 + 88 215 + 119 210 = 259 129 Гб```

### 5. Выбор прочих технологий

#### Frontend
Фронтенд будет написан с использованием технологий HTML, CSS, Typescript.
В качестве библиотеки для работы с интерфейсаими будет использоваться React, так как он обеспечивает модульность кода 
и высокую производительность, а также, прост в освоении. Для увеличения уровня абстракции при работе с css, 
воспользуемся Scss(Sass) - метаязыком, основанным на css. В качестве сборщика модулей будем использовать один из наиболее
распространенных инструментов - Webpack

#### Backend
Бэкенд будет иметь микросервисную архитектуру, что позволит легко масштабировать конкретный сервис, а не все приложение,
а также, повысит отказоустойчивость системы. В качестве основного языка будет использоваться Golang, так как он 
обладает строгой типизацией, низким порогом вхождения, многопоточностью и кроссплатформенностью "из коробки", а также, 
высокой производительностью.

#### Протоколы взаимодействия
Для общения между клиентом и сервером выберем протокол HTTP2, так как он бинарный, а также, поддеожиывет одновременную
загрузку большого количества файлов. Сами данные будут передаваться в формате json, так как он прост и поддерживается 
многими языками программирования.

На бэкенде для общения между микросервисами будет использоваться протокол gRPC, а данные будут передаваться в формате 
protobuf, что позволит увеличить скорость общения, так как структуры в protobuf быстрее парсятся и имеют меньший 
размер, в сравнении с другими форматами.

#### Балансировка нагрузки и раздача статики
Для раздачи статики м балансировки нагрузки будет использоваться Nginx, как надежный и высокопроизводительный веб-сервер,
поддерживающий возможность написания собственных модулей, а также, обладающий крупным сообществом.

### 6. Расчет нагрузки и потребного оборудования

#### Балансировка и раздача статики

* Приняв, что применяя nginx для балансировки, с учетом ssl-терминации, он способен держать ~250 RPS на ядро, получим, 
  что для балансировки 939 815 RPS для HTTPS соединений нужно ```939 815 / (250 * 36) = 105```сервер.
* Для раздачи 1089 Гб/сек, учитывая, что будут использоваться сетевые карточки на 10 Гб/сек, необходимо 109 серверов
* Считая, что в среднем, сервисом пользуется ~6000 человек в секунду, согласно [источнику](https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/), 
очевидно, что 110 серверов вполне достаточно для поддержания необходимого значения CPS.
  
Возьмем наибольшее из полученных значений и, с учетом запаса в 30 - 40%, получим, что для балансировки и раздачи статики 
требуется 145 серверов со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|32|128|10|

#### Сервис авторизации/регистрации

* Согласно расчетам, проведенным в п.2, сервис авторизации должен держать нагрузку в 1 814 217 RPS. Предположив, что 
  Предположим, что наш сервис авторизации/регистрации, написанный на Golang будет дердать нагрузку ~ 5000 RPS на ядро.
  Тогда для стабильной работы понадобится ```1 814 217 / 5000 / 36 = 11``` серверов с 36-ядерными процессорами.
* Согласно расчетам, проведенным в п.2, нагрузка на сеть для сервиса авторизации составляет 2.1 Гбит/с. Поставив на сервера
  сетевую карту на 1 Гбит/с, получим, что для поддержания работы вполне хватит 11 серверов.

С учетом запаса в 30 - 40%, получим, что для сервиса авторизации/регистрации требуется 15 серверов со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|32|128|1|

* Для хранения данных пользователей необходимо 1040 Гб. Если использовать SSD на 2 Тб, тогда для хранения данных хватит
  1 сервера c PostgreSql.
* Согласно расчетам, проведенным в п.2, сервис авторизации непосредственно для запросов авторизации/регистрации должен 
  держать нагрузку в 23 151 RPS + ```(26 * 400 000 000) / (24 * 60 * 60) = 120 371 RPS``` на запрос подписчиков при 
  формировании ленты. Итого 143 522 RPS. Приняв, что средний RPS на ядро для PostgreSql ~ 500 RPS, получаем, что необходимо 
  ```143 522 / 500 / 72 = ~ 4``` сервера.
* Согласно расчетам, проведенным в п.2, нагрузка на сеть для БД сервиса авторизации составляет 28 Мбит/с.
  Если принять, что пользователь в среднем подписан на 5000 человек, получим, что для передачи данных для запроса на 
  формирование ленты нужно ~20 Кб, тогда рассчитаем трафик: ```(20 кб * 400 000 000) * (8/1024/1024) / (24 * 60 * 60) = ~ 0.7 Гб``` 
  Поставив на сервер сетевую карту на 1 Гб/сек, получим, что для поддержания работы требуется 1 сервер.

Для обеспечения отказоустойчивости, возьмем 4 сервера. С учетом 1 master 2 slave, получаем 12 серверов со следующими
конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Tb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|72|32|1|1|

#### Хранение сессий

* Для хранения сессий пользователя будет использоваться Redis. Оценивая размер одной записи пары ключ-значение в 128 байт,
получим, что для хранения информации о всех пользователях, нужно ```128 * 1 000 000 000 / 1024^3 = ~ 120 Гб```. Такой объем 
памяти можно поместить в 1 сервер.
* Согласно расчетам, проведенным в п.2, сервис авторизации должен держать нагрузку в 1 814 217 RPS. Приняв, что средний RPS
на ядро для Redis ~ 10 000 RPS, также получаем, что хватит 6 серверов.
* Нагрузка на сеть составляет примерно 2.1 Гбит/с. Поставив на сервера
сетевые карты на 1 Гб/сек, получим, что для поддержания работы требуется 3 сервера.
  
Для обеспечения отказоустойчивости, возьмем 6 серверов. С учетом 1 master 2 slave, получаем 18 серверов со следующими 
конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Mbps)|
|:---:|:---:|:---:|:---:|
|36|256|128|100|

#### Сервис ленты и создания постов

* Согласно расчетам, проведенным в п.2, сервис ленты и создания постов должен держать нагрузку в 1 004 630 RPS + 1158 RPS.
  Предположим, что из-за вычислений генерации ленты для пользователя, на одно ядро приходится 200 RPS. Тогда рассчитаем
  количество серверов для сервиса с учетом использования 36-ядерных процессоров: ```1 005 788 / 200 / 36 = 140``` сервера.
* Согласно расчетам, проведенным в п.2, нагрузка на сеть для сервиса авторизации составляет 111 Гбит/с. Поставив на сервера
  сетевую карту на 10 Гбит/с, получим, что для поддержания работы вполне хватит 6 серверов.

С учетом запаса в 30 - 40%, получим, что для сервиса ленты требуется 182 серверов со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|32|128|10|

* Для хранения постов пользователей необходимо 49 472 Гб. Рассчитаем, какой необходим запас по памяти на 3 года
  работы сервиса: т.к. в день публикуется 100 000 000 постов, а запись в таблице постов занимает 332 байт получаем:
  ```100 000 000 * 340 * 365 * 3 / 1024^3 = ~ 34 674 Гб```

  Тогда для хранения данных с запасом нужно ```50 664 + 34 674 = 85 338 Гб```.
  Считая, что будут использоваться SSD по 4 Тб, получаем что
  для хранения данных нужно ```85 338 / (4 * 1024) = 21``` сервера.
* Согласно расчетам, проведенным в п.3, сервис комментариев должен держать нагрузку в 1 004 630 RPS.
  Исходя из предположения, что средний RPS на ядро для
  PostgreSql ~ 500 RPS, получаем, что необходимо ```1 005 788 / 500 / 72 = ~ 28``` сервера с PostgreSql.
* Согласно расчетам, проведенным в п.3, нагрузка на сеть для сервиса авторизации составляет
  111 Гбит/с. Поставив на сервера сетевые карты на 10 Гб/сек, получим,
  что для поддержания работы требуется 11 серверов.

Для обеспечения отказоустойчивости, возьмем 30 сервера. С учетом 1 master 2 slave, получаем 90 серверов со следующими
конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Tb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|72|128|4|10|

* На сервере Tarantool будем хранить данные за последние 30 дней. Тогда рассчитаем количество записей за этот срок:
  ```100 000 000 * 30 = 3 000 000 000```. Один комментарий занимает 332 байт = 0,325 Кб. Для обеспечения
  отказоустойчивости, будем использовать master-slave репликацию с 2 слейвами. Тогда, воспользовавшись
  [калькулятором](https://www.tarantool.io/ru/sizing_calculator/) Tarantool для расчетов, получаем что нужно 33 сервера
  (с учетом репликации) со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|256|256|10|

#### Сервис комментариев

* Согласно расчетам, проведенным в п.2, сервис комментариев должен держать нагрузку в 782 778 RPS. Предположив, что
  Предположим, что наш сервис комментариев, написанный на Golang будет держать нагрузку ~ 5000 RPS на ядро.
  Тогда для стабильной работы понадобится ```782 778 / 5000 / 36 = 5``` серверов с 36-ядерными процессорами.
* Согласно расчетам, проведенным в п.2, нагрузка на сеть для сервиса авторизации составляет 36 Гбит/с. Поставив на сервера
  сетевую карту на 10 Гбит/с, получим, что для поддержания работы вполне хватит 11 серверов.

С учетом запаса в 30 - 40%, получим, что для сервиса авторизации/регистрации требуется 8 серверов со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|32|128|10|

* Для хранения комментариев пользователей необходимо 88 215 Гб. Рассчитаем, какой необходим запас по памяти на 3 года 
  работы сервиса: т.к. в день публикуется 86 400 000 комментариев, а запись в таблице постов занимает 148 байт получаем:
    ```432 000 000 * 148 * 365 * 3 / 1024^3 = ~ 65 202 Гб```
 
  Тогда для хранения данных с запасом нужно ```88 215 + 65 202 = 153 417 Гб```.
  Считая, что в сервер можно поставить 2 диска, и будут использоваться SSD по 54 Тб, получаем что
  для хранения данных нужно ```153 417 / (4 * 1024) / 2 = 16``` сервера.
* Согласно расчетам, проведенным в п.3, сервис комментариев должен держать нагрузку в 782 778 RPS.
  Исходя из предположения, что средний RPS на ядро для
  PostgreSql ~ 500 RPS, получаем, что необходимо ```782 778 / 500 / 72 = ~ 22``` сервера с PostgreSql.
* Согласно расчетам, проведенным в п.3, нагрузка на сеть для сервиса авторизации составляет
  36 Гбит/с. Поставив на сервера сетевые карты на 10 Гб/сек, получим,
  что для поддержания работы требуется 4 сервера.

Для обеспечения отказоустойчивости, возьмем 22 сервера. С учетом 1 master 2 slave, получаем 66 серверов со следующими
конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Tb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|72|128|4 x 2|10|

* На сервере Tarantool будем хранить данные за последние 15 дней. Тогда рассчитаем количество записей за этот срок:
  ```5000 * 60 * 60 * 24 * 15 = 6 480 000 000```. Один комментарий занимает 148 байт = 0,145 Кб. Для обеспечения 
  отказоустойчивости, будем использовать master-slave репликацию с 2 слейвами. Тогда, воспользовавшись 
  [калькулятором](https://www.tarantool.io/ru/sizing_calculator/) Tarantool для расчетов, получаем что нужно 27 серверов
  (с учетом репликации) со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|36|256|256|10|

#### Сервис лайков

* Согласно расчетам, проведенным в п.2, сервис комментариев должен держать нагрузку в 42 500 RPS. Предположив, что
  Предположим, что наш сервис комментариев, написанный на Golang будет держать нагрузку ~ 5000 RPS на ядро.
  Тогда для стабильной работы понадобится 1 сервер с 16-ядерными процессором.
* Согласно расчетам, проведенным в п.2, нагрузка на сеть для сервиса лайков составляет 24.4 Мбит/с. Поставив на сервера
  сетевую карту на 100 Мбит/с, получим, что для поддержания работы вполне хватит 1 сервера.

Для обеспечения отказоустойчивости, возьмем 2 сервера со следующей конфигурацией:

|CPU(cores)|RAM (Gb)|SSD (Gb)|Network (Mbps)|
|:---:|:---:|:---:|:---:|
|16|32|128|100|

* Для хранения лайков пользователей необходимо 119 210 Гб. Рассчитаем, какой необходим запас по памяти на 3 года
  работы сервиса: т.к. в день публикуется 86 400 000 комментариев, а запись в таблице постов занимает 148 байт получаем:
  ```3 672 000 000 * 8 * 365 * 3 / 1024^3 = ~ 29 958 Гб```

  Тогда для хранения данных с запасом нужно ```119 210 + 29 958 = 149 168 Гб```.
  Считая, что в сервер можно поставить 8 диска, и будут использоваться SSD по 4 Тб, получаем что
  для хранения данных нужно ```149 168 / (4 * 1024) / 8 = 5``` сервера.
* Согласно расчетам, проведенным в п.3, сервис комментариев должен держать нагрузку в 42 500 RPS.
  Исходя из предположения, что средний RPS на ядро для
  PostgreSql ~ 500 RPS, получаем, что необходимо ```42 500 / 500 / 36 = ~ 3``` сервера с PostgreSql.
* Согласно расчетам, проведенным в п.3, нагрузка на сеть для сервиса авторизации составляет
  24.4 Мбит/с. Поставив на сервера сетевые карты на 100 Мб/сек, получим,
  что для поддержания работы хватит 1 сервера.

Для обеспечения отказоустойчивости, возьмем 6 сервера. С учетом 1 master 2 slave, получаем 18 серверов со следующими
конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Tb)|Network (Mbps)|
|:---:|:---:|:---:|:---:|
|36|128|4 x 8|100|

#### Хранение фотографий

* Для раздачи фото будем использовать nginx. Тогда исходя из [документации](https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/) nginx,
  приняв, что в процессоре 24 ядра, можем подсчитать, что для балансировки 8 101 802 RPS для HTTPS соединений
  нужно не менее ```8 101 802 / 10 274 = ~ 789```серверов.

* Необходимо раздавать 8 030 Гбит/c, при использовании обычных сетевых карт на 10 Гб/сек, понадобится около 1606 
  серверов. Чтобы уменьшить это количество, можем ставить сетевые карточки на 40 Гб/сек 
  (например, Intel XL710 40 GbE QSFP+ (rev 01)), тогда будет необходимо не менее 201 сервера.

* Для обеспечения отказоустойчивости и целостности данных воспользуемся технологией RAID 10, она также обеспечит 
  достаточную производительность. Для этого потребуется в 4 раза больше дисков.

* Для хранения фотографий пользователей, согласно п.4 необходимо 19 836 426 Гб. Рассчитаем, какой необходим запас по 
  памяти на 3 года работы сервиса. Учитывая, что ежедневно публикуется 100 млн. постов, и считая, что в среднем в посте 
  будет публиковаться по 1 фото, а средний размер фото равен 130 кб, получим: ```100 000 000 * 2 * 365 * 3 / 1024 = 13 575 554 Гб```
  Тогда всего выйдет ```19 836 426 + 13 575 554 = 33 411 980 Гб```.
  Тогда с учетом использования RAID 10, для хранения фото понадобится ```2 * 33 411 980 / (4 * 1024) = 65 258``` дисков
  объемом 2 TB. Учитывая, что средняя скорость чтения с SSD диска ~ 700 мбит/с, можем рассчитать минимальное количество 
  дисков для нормальной отдачи картинок. ```8030 * 1024 / 700 = 11 747``` дисков, т.е. проблем со скоростью отдачи не 
  предвидится. Если использовать стойки по 90 дисков, то понадобится ```65 258 / 90 = 726``` серверов. Этого количества
  серверов хватает для нормальной работы всей системы.
  
Итого, взяв наибольшее из получившихся значений получаем 789 серверов с конфигурациями:

|CPU(cores)|RAM (Gb)|SSD (Tb)|Network (Gbps)|
|:---:|:---:|:---:|:---:|
|24|32|2 x 90|40|

Итого:

|                                 |   CPU   |  RAM  |  SSD  | Network | Кол-во |
|---------------------------------|---------|-------|-------|---------|--------|
| Балансировка и статика          | 36 cores | 32GB | 128GB | 10Gbps | 145 |
| Сервис авторизации/регистрации  | 36 cores | 32GB | 128Gb | 10Gbps | 15 |
| Сервис авторизации/регистрации(Postgre)  | 72 cores | 32GB | 1Tb | 1Gbps | 12 |
| Хранение сессий                 | 36 cores | 256GB | 128GB | 100Mbps | 18 |
| Сервис ленты                    | 36 cores | 32GB | 128GB | 10Gbps | 182 |
| Сервис ленты(Postgre)                 | 72 cores | 128GB | 4TB | 10Gbps | 90 |
| Сервис ленты(Tarantool)                    | 36 cores | 256GB | 256GB |10Gbps | 33 |
| Сервис комментариев                    | 36 cores | 32GB | 128GB | 10Gbps | 8 |
| Сервис комментариев(Postgre)                 | 72 cores | 128GB | 4TB x 2 | 10Gbps | 66 |
| Сервис комментариев(Tarantool)                    | 36 cores | 256GB | 256GB | 10Gbps | 27 |
| Сервис лайков | 16 cores | 32GB | 128Gb | 100Mbps | 2 |
| Сервис лайков(Postgre)  | 36 cores | 128GB | 4Tb x 8 | 100Mbps | 18 |
| Хранение фотографий             | 24 cores | 32GB | 2Tb x 90 |40Gbps | 789 |
| Итого                           | | | | |  1405  |

### 7. Выбор хостинга / облачного провайдера и расположения серверов

Так как облачные решения и аренда оборудования для таких больших проектов стоят слишком дорого, бедем использовать свои 
датацентры. Так как клиенты обращаются в основном с территории Европы и Америки, расположим по одному датацентру в этих 
регионах. Тогда можем расположить один датацентр во Франкфурте и один в Нью-Йорке и поделить сервера между ними пополам.

### 8. Балансировка

Для балансировки нагрузки между датацентрами будем использовать Geo-DNS-балансировку. Это позволит обеспечить наименьший 
latency для пользователей из разных регионов. Также, для 
ограничения доступа с не отвечающих IP, можно написать скрипт по удалению таких соединений. Внутри датацентров будет 
применяться L7 балансировка, позволяющая решить проблему медленных клиентов, с использованием nignx'ов со встроенными 
настройками SSL-терминации, т.е. защищенное соединение будет устанавливаться только с серверами-балансировщиками, а при 
отправлении на узел, запрос защищен не будет.

### 9. Обеспечение отказоустойчивости

Отказоустойчивость достигается благодаря следующим решениям:
* использование микросервисной архитектуры
* использование технологии RAID 10 для хранения фотографий пользователей
  * повышение допустимого количества вышедших из строя дисков от 1 дл N/2
  * информация не потеряется, если выйдут из строя диски в пределах разных зеркал
  * повышенная надежность, скорость чтения и скорость записи
* использование master-slave репликации
* фронтенд и бэкенд будут работать в k8s, что обеспечит повышенную отказоустойчивость за счет равномерности 
  распределения мощностей
* мониторинг метрик для оповещения в случае возникновения проблем